{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k67M5q1lJui-"
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "\n",
        "import json\n",
        "import zipfile\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\n",
        "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, silhouette_score, davies_bouldin_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from typing import List, Tuple, Dict\n",
        "import logging\n",
        "from joblib import Parallel, delayed\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D43NIMlIJ1_e"
      },
      "outputs": [],
      "source": [
        "#@title Load and pre-process data\n",
        "\n",
        "def load_and_preprocess_data(dataset_choice: str = 'ransomware_headers') -> tuple[pd.DataFrame, pd.Series]:\n",
        "    \"\"\"\n",
        "    Load and preprocess the data from either the Ransomware_headers.csv file or the RISS dataset.\n",
        "\n",
        "    Args:\n",
        "        dataset_choice (str): Choice of dataset ('ransomware_headers' or 'riss').\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, pd.Series]: Features (X) and target variable (y).\n",
        "    \"\"\"\n",
        "    if dataset_choice == 'pe':\n",
        "        return load_ransomware_headers()\n",
        "    elif dataset_choice == 'riss':\n",
        "        return load_riss_dataset()\n",
        "    else:\n",
        "        raise ValueError(\"Invalid dataset choice. Choose 'ransomware_headers' or 'riss'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WhlgmWvmJ5AV"
      },
      "outputs": [],
      "source": [
        "#@title Load PE Dataset\n",
        "\n",
        "def load_ransomware_headers() -> tuple[pd.DataFrame, pd.Series]:\n",
        "    \"\"\"Load and preprocess the Ransomware_headers.csv dataset.\"\"\"\n",
        "    logging.info(\"Loading data from Ransomware_headers.csv\")\n",
        "    print(\"Loading data from Ransomware_headers.csv\")\n",
        "    df = pd.read_csv('/PE-Dataset/Ransomware_headers.csv')\n",
        "\n",
        "    # Remove irrelevant columns (adjust as needed)\n",
        "    df = df.drop(df.columns[[0, 1, 3]], axis=1)\n",
        "\n",
        "    # Separate features and target\n",
        "    X = df.iloc[:, 1:]\n",
        "    y = df.iloc[:, 0]\n",
        "\n",
        "    return preprocess_features(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ybcCGCxhJ7O1"
      },
      "outputs": [],
      "source": [
        "#@title Load RISS Dataset\n",
        "\n",
        "def load_riss_dataset() -> tuple[pd.DataFrame, pd.Series]:\n",
        "    \"\"\"Load and preprocess the RISS dataset.\"\"\"\n",
        "    logging.info(\"Loading data from RISS dataset\")\n",
        "    print(\"Loading data from RISS dataset\")\n",
        "\n",
        "    # Clone the repository if it doesn't exist\n",
        "    if not os.path.exists('riss'):\n",
        "        os.system('git clone https://github.com/rissgrouphub/ransomwaredataset2016 riss')\n",
        "\n",
        "    zip_file_path = 'riss/RansomwareData.zip'\n",
        "    extracted_folder = 'riss/data/'\n",
        "\n",
        "    # Extract the dataset if it hasn't been extracted\n",
        "    if not os.path.exists(extracted_folder):\n",
        "        os.makedirs(extracted_folder, exist_ok=True)\n",
        "        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(extracted_folder)\n",
        "\n",
        "    # Load header mapping\n",
        "    header_file = 'riss/VariableNames.txt'\n",
        "    header_mapping = {}\n",
        "    with open(header_file, 'r') as file:\n",
        "        for line in file:\n",
        "            parts = line.strip().split(';')\n",
        "            index = parts[0]\n",
        "            column_name = ';'.join(parts[1:])\n",
        "            header_mapping[int(index) - 1] = column_name\n",
        "\n",
        "    # Load the dataset\n",
        "    df = pd.read_csv('riss/data/RansomwareData.csv', header=None)\n",
        "    df.columns = [header_mapping.get(i, f'Unknown_{i}') for i in range(len(df.columns))]\n",
        "\n",
        "    df = df.drop(df.columns[0], axis=1)\n",
        "    df = df.drop(df.columns[1], axis=1)\n",
        "\n",
        "    # Separate features and target\n",
        "    X = df.iloc[:, 1:]  # The features start from column 1\n",
        "    y = df.iloc[:, 0]   # The label is in column 0\n",
        "\n",
        "    return preprocess_features(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mG40wSjQKBT2"
      },
      "outputs": [],
      "source": [
        "#@title Preprocess Features\n",
        "\n",
        "def preprocess_features(X: pd.DataFrame, y: pd.Series) -> tuple[pd.DataFrame, pd.Series]:\n",
        "    \"\"\"Preprocess the features.\"\"\"\n",
        "    # Check for missing values\n",
        "    missing_values = X.isnull().sum()\n",
        "    if missing_values.sum() > 0:\n",
        "        logging.warning(f\"Missing values found:\\n{missing_values[missing_values > 0]}\")\n",
        "        print(f\"Missing values found:\\n{missing_values[missing_values > 0]}\")\n",
        "        # Impute missing values\n",
        "        imputer = SimpleImputer(strategy='mean')\n",
        "        X = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
        "\n",
        "    # Normalize features\n",
        "    scaler = StandardScaler()\n",
        "    X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
        "\n",
        "    logging.info(f\"Data preprocessed. Shape: {X.shape}\")\n",
        "    print(f\"Data preprocessed. Shape: {X.shape}\")\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FoZef5ROKFjG"
      },
      "outputs": [],
      "source": [
        "#@title Find Optimal Features\n",
        "\n",
        "def find_optimal_features(X: pd.DataFrame, y: pd.Series, clf, max_features: int = 1000, step: int = 50) -> int:\n",
        "    \"\"\"\n",
        "    Find the optimal number of features using cross-validation.\n",
        "\n",
        "    Args:\n",
        "        X (pd.DataFrame): Features.\n",
        "        y (pd.Series): Target variable.\n",
        "        clf: Classifier to use for evaluation.\n",
        "        max_features (int): Maximum number of features to consider.\n",
        "        step (int): Step size for feature count.\n",
        "\n",
        "    Returns:\n",
        "        int: Optimal number of features.\n",
        "    \"\"\"\n",
        "    logging.info(\"Finding optimal number of features\")\n",
        "    print(\"Finding optimal number of features\")\n",
        "    feature_counts = range(step, min(X.shape[1], max_features) + 1, step)\n",
        "    mean_scores = []\n",
        "    std_scores = []\n",
        "\n",
        "    for k in feature_counts:\n",
        "        selector = SelectKBest(score_func=mutual_info_classif, k=k)\n",
        "        X_selected = selector.fit_transform(X, y)\n",
        "        scores = cross_val_score(clf, X_selected, y, cv=5, scoring='accuracy')\n",
        "        mean_scores.append(scores.mean())\n",
        "        std_scores.append(scores.std())\n",
        "        logging.info(f\"Features: {k}, Mean accuracy: {scores.mean():.4f}, Std: {scores.std():.4f}\")\n",
        "        print(f\"Features: {k}, Mean accuracy: {scores.mean():.4f}, Std: {scores.std():.4f}\")\n",
        "\n",
        "    optimal_k = feature_counts[np.argmax(mean_scores)]\n",
        "\n",
        "    # Plot the results\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.errorbar(feature_counts, mean_scores, yerr=std_scores, capsize=5)\n",
        "    plt.xlabel('Feature Count', fontsize=12)\n",
        "    plt.ylabel('Cross-validation Accuracy', fontsize=12)\n",
        "    # plt.title('Feature Selection: Accuracy vs Number of Features')\n",
        "    plt.axvline(x=optimal_k, color='r', linestyle='--', label=f'Optimal features: {optimal_k}')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.savefig('feature_selection_plot.eps')\n",
        "    plt.close()\n",
        "\n",
        "    logging.info(f\"Optimal number of features: {optimal_k}\")\n",
        "    print(f\"Optimal number of features: {optimal_k}\")\n",
        "    return optimal_k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CO--9F-hKIAF"
      },
      "outputs": [],
      "source": [
        "#@title Perform Feature Selection\n",
        "\n",
        "def perform_feature_selection(X: pd.DataFrame, y: pd.Series, k: int) -> pd.DataFrame:\n",
        "    \"\"\"Perform feature selection using mutual information.\"\"\"\n",
        "    logging.info(f\"Performing feature selection. Selecting top {k} features\")\n",
        "    print(f\"Performing feature selection. Selecting top {k} features\")\n",
        "    selector = SelectKBest(score_func=mutual_info_classif, k=min(k, X.shape[1]))\n",
        "    X_new = selector.fit_transform(X, y)\n",
        "    selected_features = X.columns[selector.get_support()]\n",
        "    print(\"Feature selection complete\")\n",
        "    return pd.DataFrame(X_new, columns=selected_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tcbHUOrKLU1"
      },
      "outputs": [],
      "source": [
        "#@title Center Attack\n",
        "\n",
        "def center_attack(X: pd.DataFrame, y: pd.Series, percent: float) -> pd.Series:\n",
        "    \"\"\"\n",
        "    Perform a center attack by flipping labels of samples closest to class centers.\n",
        "\n",
        "    Args:\n",
        "        X (pd.DataFrame): Features.\n",
        "        y (pd.Series): Target variable.\n",
        "        percent (float): Percentage of labels to flip.\n",
        "\n",
        "    Returns:\n",
        "        pd.Series: Attacked labels.\n",
        "    \"\"\"\n",
        "    if percent <= 0.0:\n",
        "        return y.copy()\n",
        "\n",
        "    logging.info(f\"Performing center attack with {percent:.2%} label flipping\")\n",
        "    print(f\"Performing center attack with {percent:.2%} label flipping\")\n",
        "    y_attacked = y.copy()\n",
        "    class_centers = [X[y == label].mean(axis=0) for label in np.unique(y)]\n",
        "    distances = np.array([np.linalg.norm(X - center, axis=1) for center in class_centers]).T\n",
        "\n",
        "    for label in np.unique(y):\n",
        "        indices = y[y == label].index\n",
        "        num_to_flip = int(len(indices) * percent)\n",
        "        flip_indices = indices[np.argsort(distances[y == label, label])[:num_to_flip]]\n",
        "        y_attacked.loc[flip_indices] = 1 - y_attacked.loc[flip_indices]\n",
        "\n",
        "    return y_attacked"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5JG3EWc9KOMW"
      },
      "outputs": [],
      "source": [
        "#@title Random Attack\n",
        "\n",
        "def random_attack(y: pd.Series, percent: float) -> pd.Series:\n",
        "    \"\"\"\n",
        "    Perform a random attack by flipping random labels.\n",
        "\n",
        "    Args:\n",
        "        y (pd.Series): Target variable.\n",
        "        percent (float): Percentage of labels to flip.\n",
        "\n",
        "    Returns:\n",
        "        pd.Series: Attacked labels.\n",
        "    \"\"\"\n",
        "    logging.info(f\"Performing random attack with {percent:.2%} label flipping\")\n",
        "    print(f\"Performing random attack with {percent:.2%} label flipping\")\n",
        "    y_attacked = y.copy()\n",
        "    num_to_flip = int(len(y) * percent)\n",
        "    flip_indices = np.random.choice(y.index, num_to_flip, replace=False)\n",
        "    y_attacked.loc[flip_indices] = 1 - y_attacked.loc[flip_indices]\n",
        "    return y_attacked"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NXs_Dav0KQzH"
      },
      "outputs": [],
      "source": [
        "#@title Evaluate Model\n",
        "\n",
        "def evaluate_model(clf, X_train: pd.DataFrame, y_train: pd.Series, X_test: pd.DataFrame, y_test: pd.Series) -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Evaluate a model using various metrics.\n",
        "\n",
        "    Args:\n",
        "        clf: Trained classifier.\n",
        "        X_train (pd.DataFrame): Training features.\n",
        "        y_train (pd.Series): Training labels.\n",
        "        X_test (pd.DataFrame): Test features.\n",
        "        y_test (pd.Series): Test labels.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, float]: Dictionary of evaluation metrics.\n",
        "    \"\"\"\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return {\n",
        "        'accuracy': accuracy_score(y_test, y_pred),\n",
        "        'f1': f1_score(y_test, y_pred),\n",
        "        'precision': precision_score(y_test, y_pred),\n",
        "        'recall': recall_score(y_test, y_pred),\n",
        "        'auc': roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1])\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3MbxwvAyKTR2"
      },
      "outputs": [],
      "source": [
        "#@title Save Results to JSON\n",
        "\n",
        "import json\n",
        "import logging\n",
        "from datetime import datetime\n",
        "\n",
        "def save_results_to_json(results: Dict, noise_levels: List[float], dataset_name: str):\n",
        "    \"\"\"Save experiment results to a JSON file with a timestamp in the file name.\"\"\"\n",
        "    # Generate a timestamp string\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "    # Create the JSON structure\n",
        "    json_results = {\n",
        "        \"dataset_name\": dataset_name,\n",
        "        \"noise_levels\": noise_levels,\n",
        "        \"results\": results\n",
        "    }\n",
        "\n",
        "    # Save the results to a file with the timestamp\n",
        "    file_name = f'/results/{dataset_name}_results_{timestamp}.json'\n",
        "    with open(file_name, 'w') as f:\n",
        "        json.dump(json_results, f)\n",
        "\n",
        "    # Log and print the save location\n",
        "    logging.info(f\"Results saved to {file_name}\")\n",
        "    print(f\"Results saved to {file_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "incLy9t4bjMu"
      },
      "outputs": [],
      "source": [
        "#@title Load Results from JSON\n",
        "\n",
        "def load_results_from_json(filename: str) -> Tuple[Dict, List[float], str]:\n",
        "    \"\"\"Load experiment results from a JSON file.\"\"\"\n",
        "    with open(filename, 'r') as f:\n",
        "        data = json.load(f)\n",
        "    return data[\"results\"], data[\"noise_levels\"], data[\"dataset_name\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PdJ8WGufbouO"
      },
      "outputs": [],
      "source": [
        "#@title Plot Results\n",
        "\n",
        "def plot_results(results: Dict, noise_levels: List[float], dataset_name: str):\n",
        "    \"\"\"Plot the results of the experiment.\"\"\"\n",
        "    metrics = ['accuracy', 'f1', 'precision', 'recall', 'auc']\n",
        "\n",
        "    for clf_name in results:\n",
        "        for metric in metrics:\n",
        "            fig, ax = plt.subplots(figsize=(5, 3.4), layout='constrained')\n",
        "\n",
        "            for attack_type in ['random', 'center']:\n",
        "                mean_values = [np.mean([r[metric] for r in results[clf_name][attack_type][str(nl)]]) for nl in noise_levels]\n",
        "                std_values = [np.std([r[metric] for r in results[clf_name][attack_type][str(nl)]]) for nl in noise_levels]\n",
        "\n",
        "                ax.errorbar(noise_levels, mean_values, yerr=std_values, capsize=5,\n",
        "                            label=f'{attack_type.capitalize()} Attack')\n",
        "\n",
        "            ax.set_xlabel('Noise Level', fontsize=12)\n",
        "            ax.set_ylabel(metric.capitalize(), fontsize=12)\n",
        "            ax.set_title(f'{metric.capitalize()} vs Noise Level for {clf_name}', fontsize=14)\n",
        "            ax.legend(fontsize=10)\n",
        "            ax.grid(True)\n",
        "            ax.tick_params(axis='both', which='major', labelsize=10)\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(f'{dataset_name}_{clf_name}_{metric}_chart.eps', dpi=300, bbox_inches='tight')\n",
        "            plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vf6NhOWbp8e"
      },
      "outputs": [],
      "source": [
        "#@title Plot Results Compact\n",
        "\n",
        "def plot_results_compact(results: Dict, noise_levels: List[float], dataset_name: str):\n",
        "    metrics = ['accuracy', 'f1', 'precision', 'recall', 'auc']\n",
        "    for clf_name in results:\n",
        "        fig, axes = plt.subplots(2, 3, figsize=(10, 6), layout='constrained')\n",
        "        axes = axes.flatten()\n",
        "        for i, metric in enumerate(metrics):\n",
        "            ax = axes[i]\n",
        "            for attack_type in ['random', 'center']:\n",
        "                mean_values = [np.mean([r[metric] for r in results[clf_name][attack_type][str(nl)]]) for nl in noise_levels]\n",
        "                std_values = [np.std([r[metric] for r in results[clf_name][attack_type][str(nl)]]) for nl in noise_levels]\n",
        "                ax.errorbar(noise_levels, mean_values, yerr=std_values, capsize=5, label=f'{attack_type.capitalize()} Attack')\n",
        "            ax.set_xlabel('Noise Level', fontsize=12)\n",
        "            ax.set_ylabel(metric.capitalize(), fontsize=12)\n",
        "            ax.set_title(f'{metric.capitalize()} vs Noise Level', fontsize=14)\n",
        "            ax.legend(fontsize=10)\n",
        "            ax.grid(True)\n",
        "            ax.tick_params(axis='both', which='major', labelsize=10)\n",
        "\n",
        "        # Remove the last (empty) subplot\n",
        "        fig.delaxes(axes[5])\n",
        "\n",
        "        fig.suptitle(f'Performance Metrics for {clf_name}', fontsize=16)\n",
        "        plt.tight_layout()\n",
        "        file_clf_name = ''\n",
        "        if clf_name == 'SVM (Linear)':\n",
        "            file_clf_name = 'svml'\n",
        "        elif clf_name == 'SVM (RBF)':\n",
        "            file_clf_name = 'svmr'\n",
        "        elif clf_name == 'Logistic Regression':\n",
        "            file_clf_name = 'lr'\n",
        "        elif clf_name == 'Neural Network':\n",
        "            file_clf_name = 'nn'\n",
        "        plt.savefig(f'{dataset_name}_{file_clf_name}_charts.eps', dpi=300, bbox_inches='tight')\n",
        "        plt.savefig(f'{dataset_name}_{file_clf_name}_charts.pdf', dpi=300, bbox_inches='tight')\n",
        "        plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tomQoFFPJof3"
      },
      "outputs": [],
      "source": [
        "#@title Plot Results Compact Both\n",
        "\n",
        "def plot_results_compact_both(results: Dict, results2: Dict, noise_levels: List[float], dataset_name: str, dataset_name2: str):\n",
        "    metrics = ['accuracy', 'f1', 'precision', 'recall', 'auc']\n",
        "    for clf_name in results:\n",
        "        for i, metric in enumerate(metrics):\n",
        "            fig, ax = plt.subplots(figsize=(6, 4))\n",
        "            for attack_type in ['random', 'center']:\n",
        "                mean_values = [np.mean([r[metric] for r in results[clf_name][attack_type][str(nl)]]) for nl in noise_levels]\n",
        "                mean_values2 = [np.mean([r[metric] for r in results2[clf_name][attack_type][str(nl)]]) for nl in noise_levels]\n",
        "                std_values = [np.std([r[metric] for r in results[clf_name][attack_type][str(nl)]]) for nl in noise_levels]\n",
        "                std_values2 = [np.std([r[metric] for r in results2[clf_name][attack_type][str(nl)]]) for nl in noise_levels]\n",
        "                ax.errorbar(noise_levels, mean_values, yerr=std_values, capsize=5, label=f'{attack_type.capitalize()} Attack {dataset_name}')\n",
        "                ax.errorbar(noise_levels, mean_values2, yerr=std_values2, capsize=5, label=f'{attack_type.capitalize()} Attack {dataset_name2}')\n",
        "\n",
        "            ax.set_xlabel('Noise Level', fontsize=12)\n",
        "            ax.set_ylabel(metric.capitalize(), fontsize=12)\n",
        "            ax.set_title(f'{metric.capitalize()} vs Noise Level', fontsize=14)\n",
        "            ax.legend(fontsize=10)\n",
        "            ax.grid(True)\n",
        "            ax.tick_params(axis='both', which='major', labelsize=10)\n",
        "\n",
        "            plt.tight_layout()\n",
        "\n",
        "            file_clf_name = ''\n",
        "            if clf_name == 'SVM (Linear)':\n",
        "                file_clf_name = 'svml'\n",
        "            elif clf_name == 'SVM (RBF)':\n",
        "                file_clf_name = 'svmr'\n",
        "            elif clf_name == 'Logistic Regression':\n",
        "                file_clf_name = 'lr'\n",
        "            elif clf_name == 'Neural Network':\n",
        "                file_clf_name = 'nn'\n",
        "\n",
        "            # Save each subplot individually\n",
        "            plt.savefig(f'/charts/{file_clf_name}_charts_{metric}.pdf', dpi=300, bbox_inches='tight')\n",
        "            plt.savefig(f'/charts/{file_clf_name}_charts_{metric}.eps', dpi=300, bbox_inches='tight')\n",
        "            plt.close(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qn8kgZHGcOze"
      },
      "outputs": [],
      "source": [
        "#@title Run Experiment\n",
        "\n",
        "def run_experiment(X: pd.DataFrame, y: pd.Series, classifiers: list[dict], noise_levels: list[float], n_repeats: int = 5, dataset_name: str = 'riss'):\n",
        "    \"\"\"Run the main experiment.\"\"\"\n",
        "    results = {}\n",
        "    print(\"Main experiment started\")\n",
        "    for clf_config in classifiers:\n",
        "        clf_name = clf_config['name']\n",
        "        print(f\"Performing attacks on {clf_name}\")\n",
        "        clf = clf_config['clf']\n",
        "        param_grid = clf_config.get('param_grid', {})\n",
        "\n",
        "        logging.info(f\"Running experiment for {clf_name}\")\n",
        "        results[clf_name] = {'random': {str(nl): [] for nl in noise_levels}, 'center': {str(nl): [] for nl in noise_levels}}\n",
        "\n",
        "        def run_iteration():\n",
        "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=np.random.randint(0, 1000))\n",
        "            if param_grid:\n",
        "                grid_search = GridSearchCV(clf, param_grid, cv=10, scoring='accuracy', n_jobs=-1)\n",
        "                grid_search.fit(X_train, y_train)\n",
        "                clf_tuned = grid_search.best_estimator_\n",
        "            else:\n",
        "                clf_tuned = clf.fit(X_train, y_train)\n",
        "\n",
        "            iteration_results = {}\n",
        "            for noise_level in noise_levels:\n",
        "                y_random = random_attack(y_train, noise_level)\n",
        "                clf_tuned.fit(X_train, y_random)\n",
        "                iteration_results[('random', str(noise_level))] = evaluate_model(clf_tuned, X_train, y_random, X_test, y_test)\n",
        "\n",
        "                y_center = center_attack(X_train, y_train, noise_level)\n",
        "                # y_center = improved_center_attack_weighted(X_train, y_train, noise_level, center_method='median', distance_metric='cosine', global_percentage=True, random_selection=True, weighting_method='variance')\n",
        "                # y_center = improved_center_attack_density(X_train, y_train, noise_level)\n",
        "                # y_center = autoencoder_based_attack(X_train, y_train, noise_level)\n",
        "\n",
        "                clf_tuned.fit(X_train, y_center)\n",
        "                iteration_results[('center', str(noise_level))] = evaluate_model(clf_tuned, X_train, y_center, X_test, y_test)\n",
        "\n",
        "            return iteration_results\n",
        "\n",
        "        all_results = Parallel(n_jobs=-1)(delayed(run_iteration)() for _ in range(n_repeats))\n",
        "\n",
        "        for iteration_result in all_results:\n",
        "            for (attack_type, noise_level), eval_result in iteration_result.items():\n",
        "                results[clf_name][attack_type][noise_level].append(eval_result)\n",
        "\n",
        "    save_results_to_json(results, noise_levels, dataset_name)\n",
        "    #plot_results_compact(results, noise_levels, dataset_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-DZxi0QGEIem"
      },
      "outputs": [],
      "source": [
        "#@title Fast Run Experiment\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import logging\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "# Assume random_attack, improved_center_attack_weighted, evaluate_model are defined elsewhere\n",
        "\n",
        "def run_experiment_fast(X: pd.DataFrame, y: pd.Series, classifiers: list[dict], noise_levels: list[float], n_repeats: int = 5, dataset_name: str = 'riss'):\n",
        "    \"\"\"Run the main experiment with speed optimizations.\"\"\"\n",
        "    results = {}\n",
        "    print(\"Main experiment started\")\n",
        "    for clf_config in classifiers:\n",
        "        clf_name = clf_config['name']\n",
        "        print(f\"Performing attacks on {clf_name}\")\n",
        "        clf = clf_config['clf']\n",
        "        param_grid = clf_config.get('param_grid', {})\n",
        "        logging.info(f\"Running experiment for {clf_name}\")\n",
        "        results[clf_name] = {'random': {str(nl): [] for nl in noise_levels}, 'center': {str(nl): [] for nl in noise_levels}}\n",
        "\n",
        "        def run_iteration(repeat_index):  # Add repeat_index for individual random states\n",
        "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42 + repeat_index)  # Use different random state for each repeat\n",
        "\n",
        "            # Train the classifier once per iteration on the clean data\n",
        "            if param_grid:\n",
        "                grid_search = GridSearchCV(clf, param_grid, cv=10, scoring='accuracy', n_jobs=-1)\n",
        "                grid_search.fit(X_train, y_train)\n",
        "                clf_tuned = grid_search.best_estimator_\n",
        "            else:\n",
        "                clf_tuned = clf.fit(X_train, y_train)\n",
        "\n",
        "            iteration_results = {}\n",
        "            for noise_level in noise_levels:\n",
        "                # Evaluate with random attack\n",
        "                y_random = random_attack(y_train, noise_level)\n",
        "                iteration_results[('random', str(noise_level))] = evaluate_model(clf_tuned, X_train, y_random, X_test, y_test)\n",
        "\n",
        "                # Evaluate with center attack\n",
        "                y_center = improved_center_attack_weighted(X_train, y_train, noise_level, center_method='median', distance_metric='cosine', global_percentage=True, random_selection=True, weighting_method='variance')\n",
        "                iteration_results[('center', str(noise_level))] = evaluate_model(clf_tuned, X_train, y_center, X_test, y_test)\n",
        "\n",
        "            return iteration_results\n",
        "\n",
        "        # Pass repeat index to run_iteration for different random states\n",
        "        all_results = Parallel(n_jobs=-1)(delayed(run_iteration)(i) for i in range(n_repeats))\n",
        "\n",
        "        for iteration_result in all_results:\n",
        "            for (attack_type, noise_level), eval_result in iteration_result.items():\n",
        "                results[clf_name][attack_type][noise_level].append(eval_result)\n",
        "\n",
        "    save_results_to_json(results, noise_levels, dataset_name)\n",
        "    # plot_results_compact(results, noise_levels, dataset_name)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Set dataset name\n",
        "dataset_choice = 'riss'\n",
        "\n",
        "# Load and preprocess data\n",
        "X, y = load_and_preprocess_data(dataset_choice)\n",
        "\n",
        "# Find optimal number of features\n",
        "base_clf = LogisticRegression(random_state=42)  # You can choose any classifier here\n",
        "optimal_k = find_optimal_features(X, y, base_clf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87VMB1jGALDd",
        "outputId": "ef3aa48f-09e1-4b0b-deaa-68e23ac7aa2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data from RISS dataset\n",
            "Data preprocessed. Shape: (1524, 30967)\n",
            "Finding optimal number of features\n",
            "Features: 50, Mean accuracy: 0.8825, Std: 0.0092\n",
            "Features: 100, Mean accuracy: 0.9075, Std: 0.0147\n",
            "Features: 150, Mean accuracy: 0.9265, Std: 0.0116\n",
            "Features: 200, Mean accuracy: 0.9331, Std: 0.0092\n",
            "Features: 250, Mean accuracy: 0.9331, Std: 0.0148\n",
            "Features: 300, Mean accuracy: 0.9377, Std: 0.0116\n",
            "Features: 350, Mean accuracy: 0.9613, Std: 0.0094\n",
            "Features: 400, Mean accuracy: 0.9370, Std: 0.0148\n",
            "Features: 450, Mean accuracy: 0.9239, Std: 0.0158\n",
            "Features: 500, Mean accuracy: 0.9436, Std: 0.0113\n",
            "Features: 550, Mean accuracy: 0.9442, Std: 0.0215\n",
            "Features: 600, Mean accuracy: 0.9495, Std: 0.0061\n",
            "Features: 650, Mean accuracy: 0.9462, Std: 0.0154\n",
            "Features: 700, Mean accuracy: 0.9475, Std: 0.0154\n",
            "Features: 750, Mean accuracy: 0.9449, Std: 0.0120\n",
            "Features: 800, Mean accuracy: 0.9423, Std: 0.0152\n",
            "Features: 850, Mean accuracy: 0.9449, Std: 0.0143\n",
            "Features: 900, Mean accuracy: 0.9416, Std: 0.0136\n",
            "Features: 950, Mean accuracy: 0.9449, Std: 0.0178\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.backends.backend_ps:The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features: 1000, Mean accuracy: 0.9665, Std: 0.0124\n",
            "Optimal number of features: 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcDTvwEgM6D_",
        "outputId": "29f3e489-7c13-41e4-98ea-bc64ee2a082e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data from Ransomware_headers.csv\n",
            "Data preprocessed. Shape: (2157, 1024)\n",
            "Performing feature selection. Selecting top 800 features\n",
            "Feature selection complete\n",
            "Main experiment started\n",
            "Performing attacks on SVM (Linear)\n",
            "Performing attacks on SVM (RBF)\n",
            "Performing attacks on Logistic Regression\n",
            "Performing attacks on Neural Network\n",
            "Results saved to /content/drive/MyDrive/Thesis/results/pe_results_20241224_074226.json\n"
          ]
        }
      ],
      "source": [
        "#@title Main\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Set dataset name\n",
        "dataset_choice = 'pe'\n",
        "\n",
        "# Load and preprocess data\n",
        "X, y = load_and_preprocess_data(dataset_choice)\n",
        "\n",
        "# Find optimal number of features\n",
        "base_clf = LogisticRegression(random_state=42)  # You can choose any classifier here\n",
        "optimal_k = 800 if dataset_choice == 'pe' else 1000 # find_optimal_features(X, y, base_clf)\n",
        "\n",
        "# Perform feature selection\n",
        "X_selected = perform_feature_selection(X, y, k=optimal_k)\n",
        "\n",
        "# Define classifiers and their hyperparameter grids\n",
        "classifiers = [\n",
        "    # {'name': 'Naive Bayes', 'clf': GaussianNB()},\n",
        "    {'name': 'SVM (Linear)', 'clf': SVC(kernel='linear', probability=True),\n",
        "      'param_grid': {'C': [0.1, 1, 10]}},\n",
        "    {'name': 'SVM (RBF)', 'clf': SVC(kernel='rbf', probability=True),\n",
        "      'param_grid': {'C': [0.1, 1, 10], 'gamma': ['scale', 'auto']}},\n",
        "    {'name': 'Logistic Regression', 'clf': LogisticRegression(random_state=42),\n",
        "      'param_grid': {'C': [0.1, 1, 10]}},\n",
        "    {'name': 'Neural Network', 'clf': MLPClassifier(random_state=42),\n",
        "      'param_grid': {'hidden_layer_sizes': [(50,), (100,), (50, 50)], 'alpha': [0.0001, 0.001, 0.01]}}\n",
        "]\n",
        "\n",
        "# Define noise levels\n",
        "noise_levels = [0.0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3]\n",
        "\n",
        "# Run the experiment\n",
        "run_experiment(X_selected, y, classifiers, noise_levels, 5, dataset_choice)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-OG8kgmecYqO",
        "outputId": "737febf1-68e8-45ae-ee25-dceab63a34af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.backends.backend_ps:The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
            "WARNING:matplotlib.backends.backend_ps:The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
            "WARNING:matplotlib.backends.backend_ps:The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
            "WARNING:matplotlib.backends.backend_ps:The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
            "WARNING:matplotlib.backends.backend_ps:The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
            "WARNING:matplotlib.backends.backend_ps:The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
            "WARNING:matplotlib.backends.backend_ps:The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
            "WARNING:matplotlib.backends.backend_ps:The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
            "WARNING:matplotlib.backends.backend_ps:The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
            "WARNING:matplotlib.backends.backend_ps:The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
            "WARNING:matplotlib.backends.backend_ps:The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
            "WARNING:matplotlib.backends.backend_ps:The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
            "WARNING:matplotlib.backends.backend_ps:The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
            "WARNING:matplotlib.backends.backend_ps:The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
            "WARNING:matplotlib.backends.backend_ps:The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
            "WARNING:matplotlib.backends.backend_ps:The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
            "WARNING:matplotlib.backends.backend_ps:The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
            "WARNING:matplotlib.backends.backend_ps:The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
            "WARNING:matplotlib.backends.backend_ps:The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
            "WARNING:matplotlib.backends.backend_ps:The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n"
          ]
        }
      ],
      "source": [
        "#@title Generate Plots from Saved Result\n",
        "\n",
        "# If you want to generate plots from saved results without running the experiment:\n",
        "results, noise_levels, dataset_name = load_results_from_json('/results/pe_results_20241224_074226.json')\n",
        "results2, noise_levels2, dataset_name2 = load_results_from_json('/results/riss_results_20241224_063619.json')\n",
        "\n",
        "plot_results_compact_both(results, results2, noise_levels, 'PE', 'RISS')\n",
        "# plot_results_compact(results, noise_levels, dataset_name)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1G7cfzFJ53QQhcYxqqT9VPBCM5zWyh1Pj",
      "authorship_tag": "ABX9TyMIFEkDt/A05Re3l03K8Tql"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}